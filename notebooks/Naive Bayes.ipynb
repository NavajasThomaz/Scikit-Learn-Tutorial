{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aabae6e7",
   "metadata": {},
   "source": [
    "# Understanding Naive Bayes Classifiers with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ad064",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of simple, yet often surprisingly effective, probabilistic classifiers based on applying **Bayes' Theorem** with strong (naive) independence assumptions between the features.\n",
    "\n",
    "They are particularly popular for text classification (like spam filtering or sentiment analysis) and work well even with high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3945ab",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "At the heart of Naive Bayes is Bayes' Theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event. For classification, we want to find the probability of a class (\\(y\\)) given a set of features (\\(\\mathbf{x} = (x_1, x_2, ..., x_n)\\)). Bayes' Theorem states:\n",
    "\n",
    "\\[ P(y | x_1, ..., x_n) = \\frac{P(x_1, ..., x_n | y) P(y)}{P(x_1, ..., x_n)} \\]\n",
    "\n",
    "Where:\n",
    "*   \\( P(y | x_1, ..., x_n) \\) is the **posterior probability**: the probability of class \\(y\\) given the observed features \\(\\mathbf{x}\\).\n",
    "*   \\( P(x_1, ..., x_n | y) \\) is the **likelihood**: the probability of observing the features \\(\\mathbf{x}\\) given that the class is \\(y\\).\n",
    "*   \\( P(y) \\) is the **prior probability**: the initial probability of class \\(y\\) before observing any features.\n",
    "*   \\( P(x_1, ..., x_n) \\) is the **evidence**: the probability of observing the features \\(\\mathbf{x}\\). This acts as a normalizing constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d5231",
   "metadata": {},
   "source": [
    "### The \"Naive\" Assumption: Conditional Independence\n",
    "\n",
    "Calculating the likelihood \\( P(x_1, ..., x_n | y) \\) directly is often computationally intractable, especially with many features. Naive Bayes simplifies this calculation by making a strong assumption: **all features are conditionally independent given the class \\(y\\)**.\n",
    "\n",
    "This means we assume that the presence or value of a particular feature does not affect the presence or value of any other feature, given the class label. Mathematically:\n",
    "\n",
    "\\[ P(x_1, ..., x_n | y) = P(x_1 | y) P(x_2 | y) \\dots P(x_n | y) = \\prod_{i=1}^{n} P(x_i | y) \\]\n",
    "\n",
    "While this assumption is often violated in real-world data (features are rarely completely independent), Naive Bayes classifiers still tend to perform well in practice, especially when the independence assumption doesn't drastically impact the relative probabilities needed for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd4192",
   "metadata": {},
   "source": [
    "### Classification Process\n",
    "\n",
    "Using the naive independence assumption, the classification rule becomes:\n",
    "\n",
    "1.  Calculate the prior probability \\( P(y) \\) for each class \\(y\\) from the training data (e.g., the proportion of training samples belonging to class \\(y\\)).\n",
    "2.  Calculate the likelihood \\( P(x_i | y) \\) for each feature \\(x_i\\) given each class \\(y\\). The method for calculating this depends on the type of Naive Bayes used (see below).\n",
    "3.  For a new data point \\(\\mathbf{x} = (x_1, ..., x_n)\\), calculate the posterior probability (or a value proportional to it) for each class \\(y\\):\n",
    "    \\[ P(y | \\mathbf{x}) \\propto P(y) \\prod_{i=1}^{n} P(x_i | y) \\]\n",
    "    (We can ignore the denominator \\( P(/mathbf{x}) \\) because it's the same for all classes and doesn't affect which class has the highest probability).\n",
    "4.  Assign the data point \\(\\mathbf{x}\\) to the class \\(y\\) that maximizes this value (Maximum A Posteriori or MAP estimation):\n",
    "    \\[ \\hat{y} = \\arg\\max_{y} P(y) \\prod_{i=1}^{n} P(x_i | y) \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569dc763",
   "metadata": {},
   "source": [
    "### Types of Naive Bayes Classifiers in Scikit-Learn\n",
    "\n",
    "Scikit-learn implements several variants of Naive Bayes, differing mainly in the assumptions they make about the distribution of \\( P(x_i | y) \\):\n",
    "\n",
    "1.  **Gaussian Naive Bayes (`GaussianNB`):**\n",
    "    *   **Assumption:** Assumes that continuous features \\(x_i\\) follow a Gaussian (normal) distribution for each class \\(y\\).\n",
    "    *   **Calculation:** The likelihood \\( P(x_i | y) \\) is calculated using the Gaussian probability density function, based on the mean and variance of feature \\(x_i\\) for samples belonging to class \\(y\\) in the training data.\n",
    "    *   **Use Case:** Suitable for continuous numerical features that are approximately normally distributed (though it often works reasonably well even if this assumption is moderately violated).\n",
    "\n",
    "2.  **Multinomial Naive Bayes (`MultinomialNB`):**\n",
    "    *   **Assumption:** Assumes features represent counts or frequencies (e.g., word counts in a document).\n",
    "    *   **Calculation:** The likelihood \\( P(x_i | y) \\) is estimated based on the frequency of feature \\(x_i\\) (e.g., word \\(i\\)) appearing in documents of class \\(y\\). It typically uses **Laplace (or Additive) smoothing** (controlled by the `alpha` parameter) to handle features that were not observed in the training data for a particular class (the zero-frequency problem).\n",
    "    *   **Use Case:** Standard choice for text classification with word counts or TF-IDF vectors.\n",
    "\n",
    "3.  **Bernoulli Naive Bayes (`BernoulliNB`):**\n",
    "    *   **Assumption:** Assumes features are binary (0 or 1), indicating the presence or absence of a feature (e.g., whether a specific word appears in a document or not).\n",
    "    *   **Calculation:** The likelihood \\( P(x_i | y) \\) is estimated based on the frequency of documents of class \\(y\\) that contain feature \\(x_i\\).\n",
    "    *   **Use Case:** Suitable for binary/boolean features. Also used in text classification, particularly with binary term occurrence vectors (presence/absence) rather than term frequencies.\n",
    "\n",
    "4.  **Complement Naive Bayes (`ComplementNB`):**\n",
    "    *   An adaptation of Multinomial NB particularly suited for **imbalanced datasets**.\n",
    "\n",
    "5.  **Categorical Naive Bayes (`CategoricalNB`):**\n",
    "    *   Suitable for **discrete categorical features** where features follow a categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702346aa",
   "metadata": {},
   "source": [
    "### Pros and Cons of Naive Bayes\n",
    "\n",
    "**Pros:**\n",
    "*   **Simple and Fast:** Very easy to implement and computationally efficient for both training and prediction.\n",
    "*   **Requires Less Training Data:** Can perform well even with relatively small datasets compared to more complex models.\n",
    "*   **Handles High Dimensions:** Works well with a large number of features (e.g., text classification with thousands of words).\n",
    "*   **Good Performance:** Often works surprisingly well despite the naive independence assumption.\n",
    "*   **Handles Irrelevant Features:** Tends to ignore irrelevant features naturally (their likelihoods \\(P(x_i|y)\\) will be similar across classes).\n",
    "\n",
    "**Cons:**\n",
    "*   **Naive Independence Assumption:** The core assumption of feature independence is often unrealistic and can limit accuracy if features are strongly correlated.\n",
    "*   **Zero-Frequency Problem:** If a feature value in the test set was never seen with a particular class in the training set, the conditional probability \\(P(x_i|y)\\) will be zero, potentially zeroing out the entire posterior probability. Smoothing techniques (like Laplace) are needed to mitigate this (especially for Multinomial/Bernoulli NB).\n",
    "*   **Sensitivity to Feature Distributions (Gaussian NB):** Gaussian NB assumes features are normally distributed, which might not hold true.\n",
    "*   **Poor Probability Estimates:** While good at classification (picking the most likely class), the actual predicted probabilities might not be very accurate or well-calibrated due to the independence assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825933e",
   "metadata": {},
   "source": [
    "### Implementation with Scikit-Learn (GaussianNB Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f61ed",
   "metadata": {},
   "source": [
    "#### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40263134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Optional for GaussianNB, see note below\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.datasets import make_classification # To generate synthetic data\n",
    "\n",
    "# Configure plots for better visualization\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except OSError:\n",
    "    plt.style.use('ggplot') # Fallback style\n",
    "    print(\"Style 'seaborn-v0_8-whitegrid' not found, using 'ggplot' instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0598b26",
   "metadata": {},
   "source": [
    "#### 2. Generate Data\n",
    "\n",
    "We'll generate synthetic data suitable for Gaussian Naive Bayes (continuous features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data (2 features for visualization)\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, random_state=42, class_sep=1.0)\n",
    "\n",
    "# Visualize the generated data\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#0000FF']), edgecolors='k', s=35)\n",
    "plt.title('Synthetic Binary Classification Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "handles, labels = scatter.legend_elements()\n",
    "plt.legend(handles=handles, labels=['Class 0', 'Class 1'])\n",
    "plt.show()\n",
    "\n",
    "# Convert to DataFrame (optional)\n",
    "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\n",
    "df['Target'] = y\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e16772",
   "metadata": {},
   "source": [
    "#### 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca556c0a",
   "metadata": {},
   "source": [
    "#### 4. Feature Scaling (Generally Not Required, but Consider for GaussianNB)\n",
    "\n",
    "Naive Bayes algorithms (especially Multinomial and Bernoulli) typically do **not** require feature scaling. They work based on probabilities and counts, not distances.\n",
    "\n",
    "However, for **Gaussian Naive Bayes**, which calculates means and variances, features with very large values might dominate the variance calculation. While not strictly necessary like for SVM or Logistic Regression, scaling *can* sometimes stabilize the variance calculation or slightly improve performance if features have vastly different scales. We'll apply it here for demonstration, but be aware it's often skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (Optional for GaussianNB)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling applied (using StandardScaler).\")\n",
    "# To run without scaling, simply use X_train and X_test directly in the fit/predict steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5552e99",
   "metadata": {},
   "source": [
    "#### 5. Training the Gaussian Naive Bayes Model\n",
    "\n",
    "We'll use scikit-learn's `GaussianNB`. It has few hyperparameters to tune compared to other models. The main one is `var_smoothing` which adds a small value to the variances for calculation stability (similar to Laplace smoothing but for variances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c68fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model on the (optionally scaled) training data\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Gaussian Naive Bayes model trained.\")\n",
    "print(f\"Classes learned: {gnb.classes_}\")\n",
    "print(f\"Class priors P(y): {gnb.class_prior_}\")\n",
    "# You can also inspect gnb.theta_ (means) and gnb.var_ (variances) per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e15a3",
   "metadata": {},
   "source": [
    "#### 6. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Test set results (class labels)\n",
    "y_pred = gnb.predict(X_test_scaled)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = gnb.predict_proba(X_test_scaled)\n",
    "\n",
    "# Display predictions vs actual values (optional)\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test, \n",
    "    'Predicted Label': y_pred, \n",
    "    'Predicted Prob (Class 0)': y_pred_proba[:, 0],\n",
    "    'Predicted Prob (Class 1)': y_pred_proba[:, 1]\n",
    "})\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(results_df.head(10).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d698b",
   "metadata": {},
   "source": [
    "#### 7. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebb52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1]) # Use probabilities for class 1 for AUC\n",
    "\n",
    "print(\"\\n--- Model Evaluation (GaussianNB) ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nAccuracy: {acc:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0d46d",
   "metadata": {},
   "source": [
    "#### 8. Visualizing the Decision Boundary (GaussianNB)\n",
    "\n",
    "Since we used 2 features, we can visualize the decision boundary learned by the Gaussian Naive Bayes model. Unlike linear models, the boundary can be non-linear (typically quadratic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot decision boundaries (reusable)\n",
    "def plot_decision_boundary(X, y, model, scaler, title):\n",
    "    X_set, y_set = X, y # Expecting scaled data\n",
    "    \n",
    "    # Create mesh grid using original data range, then scale it\n",
    "    X_orig = scaler.inverse_transform(X_set)\n",
    "    x_min, x_max = X_orig[:, 0].min() - 1, X_orig[:, 0].max() + 1\n",
    "    y_min, y_max = X_orig[:, 1].min() - 1, X_orig[:, 1].max() + 1\n",
    "    \n",
    "    xx_orig, yy_orig = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                                   np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    mesh_points = np.c_[xx_orig.ravel(), yy_orig.ravel()]\n",
    "    mesh_points_scaled = scaler.transform(mesh_points)\n",
    "    \n",
    "    # Predict the class for each point in the scaled mesh grid\n",
    "    Z = model.predict(mesh_points_scaled)\n",
    "    Z = Z.reshape(xx_orig.shape)\n",
    "\n",
    "    # Plot the contour and the data points (using original scale for axes)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    plt.contourf(xx_orig, yy_orig, Z, alpha=0.4, cmap=cmap_light)\n",
    "    plt.xlim(xx_orig.min(), xx_orig.max())\n",
    "    plt.ylim(yy_orig.min(), yy_orig.max())\n",
    "\n",
    "    # Plot the actual data points (using original scale)\n",
    "    scatter = plt.scatter(X_orig[:, 0], X_orig[:, 1], c=y_set, cmap=cmap_bold, edgecolors='k', s=35)\n",
    "    \n",
    "    # Create legend\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    plt.legend(handles=handles, labels=['Class 0', 'Class 1'], title=\"Classes\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1 (Original Scale)')\n",
    "    plt.ylabel('Feature 2 (Original Scale)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Training Set\n",
    "plot_decision_boundary(X_train_scaled, y_train, gnb, scaler, 'Gaussian Naive Bayes Decision Boundary (Training set)')\n",
    "\n",
    "# Plot for Test Set\n",
    "plot_decision_boundary(X_test_scaled, y_test, gnb, scaler, 'Gaussian Naive Bayes Decision Boundary (Test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a366db",
   "metadata": {},
   "source": [
    "The decision boundary reflects the regions where the posterior probability for one class becomes higher than for the other, based on the learned Gaussian distributions for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4473f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook explored the Naive Bayes algorithm, focusing on the Gaussian variant.\n",
    "\n",
    "Key takeaways:\n",
    "*   Naive Bayes classifiers are based on **Bayes' Theorem** with a **naive assumption of feature independence**.\n",
    "*   Different variants exist (`GaussianNB`, `MultinomialNB`, `BernoulliNB`, etc.) based on assumptions about feature distributions.\n",
    "*   They are **simple, fast, and efficient**, especially for high-dimensional data like text.\n",
    "*   **Feature scaling is generally not required**, although it might sometimes be considered for `GaussianNB`.\n",
    "*   **Smoothing** (e.g., `alpha` in `MultinomialNB`/`BernoulliNB`, `var_smoothing` in `GaussianNB`) is important to handle unseen feature values or improve numerical stability.\n",
    "*   Despite the often unrealistic independence assumption, Naive Bayes frequently provides good baseline performance for classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
